{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7413c2e",
   "metadata": {},
   "source": [
    "## EmbeddingLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3cddcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary: 10000 words\n",
      "Chinese vocabulary: 10000 words\n"
     ]
    }
   ],
   "source": [
    "from vector_mapping import EmbeddingLoader\n",
    "\n",
    "EMBEDDING_PATH_ROOT = \"./pretrained_word2vec/\"\n",
    "EN_EMBEDDING = \"en/GoogleNews-vectors-negative300.bin.gz\"\n",
    "ZH_EMBEDDING = \"zh/sgns.merge.word.bz2\"\n",
    "\n",
    "MAX_VOCAB = 10000\n",
    "\n",
    "# Create separate loaders for each language (new stateful API requires separate instances)\n",
    "en_loader = EmbeddingLoader()\n",
    "zh_loader = EmbeddingLoader()\n",
    "\n",
    "# Load embeddings\n",
    "en_loader.load_word2vec(filepath=EMBEDDING_PATH_ROOT+EN_EMBEDDING, max_vocab=MAX_VOCAB)\n",
    "zh_loader.load_word2vec(filepath=EMBEDDING_PATH_ROOT+ZH_EMBEDDING, max_vocab=MAX_VOCAB)\n",
    "\n",
    "# Get embeddings dicts for use with helper functions below\n",
    "en_emb = en_loader.get_embeddings()\n",
    "zh_emb = zh_loader.get_embeddings()\n",
    "\n",
    "# Show vocabulary sizes\n",
    "print(f\"English vocabulary: {len(en_loader)} words\")\n",
    "print(f\"Chinese vocabulary: {len(zh_loader)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vk5ds9g8lw",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_word_embedding(word, embeddings):\n",
    "    if word not in embeddings:\n",
    "        raise KeyError(f\"Word '{word}' not found in embeddings\")\n",
    "    return embeddings[word]\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Calculate dot product\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Calculate magnitudes (L2 norms)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d1dbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11028192\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(get_word_embedding(\"一\", zh_emb), get_word_embedding(\"one\", en_emb))\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62160185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.684622\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(get_word_embedding(\"一\", zh_emb), get_word_embedding(\"一个\", zh_emb))\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e156f422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4903487\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(get_word_embedding(\"one\", en_emb), get_word_embedding(\"single\", en_emb))\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fk6suwa457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word existence checks:\n",
      "  'cat' in en_loader: True\n",
      "  'dog' in en_loader: True\n",
      "  'asdfghjkl' in en_loader: False\n",
      "\n",
      "Get single word embedding:\n",
      "  'cat' embedding shape: (300,)\n",
      "  'cat' embedding (first 10 dims): [ 0.0123291   0.20410156 -0.28515625  0.21679688  0.11816406  0.08300781\n",
      "  0.04980469 -0.00952148  0.22070312 -0.12597656]\n",
      "\n",
      "Vocabulary operations:\n",
      "  English vocab sample: ['delivers', 'Jerry', 'scheduled', 'South', 'Venezuelan', 'detection', 'Wi_Fi', 'Elsewhere', 'colorful', 'readers']\n",
      "  Chinese vocab sample: ['花钱', '修正主义', '日中', '未经', '大奖赛', '骑兵', '记者', '率领', '经济体', '均价']\n",
      "\n",
      "Method chaining example:\n",
      "  Filtered vocab size: 2\n",
      "  Filtered words: {'car', 'house'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Word existence checks:\")\n",
    "print(f\"  'cat' in en_loader: {'cat' in en_loader}\")\n",
    "print(f\"  'dog' in en_loader: {'dog' in en_loader}\")\n",
    "print(f\"  'asdfghjkl' in en_loader: {'asdfghjkl' in en_loader}\")\n",
    "print()\n",
    "\n",
    "print(\"Get single word embedding:\")\n",
    "cat_vec = en_loader.get_embedding('cat')\n",
    "print(f\"  'cat' embedding shape: {cat_vec.shape}\")\n",
    "print(f\"  'cat' embedding (first 10 dims): {cat_vec[:10]}\")\n",
    "print()\n",
    "\n",
    "print(\"Vocabulary operations:\")\n",
    "en_vocab = en_loader.get_vocabulary()\n",
    "zh_vocab = zh_loader.get_vocabulary()\n",
    "print(f\"  English vocab sample: {list(en_vocab)[:10]}\")\n",
    "print(f\"  Chinese vocab sample: {list(zh_vocab)[:10]}\")\n",
    "print()\n",
    "\n",
    "print(\"Method chaining example:\")\n",
    "test_loader = EmbeddingLoader()\n",
    "test_loader.load_word2vec(\n",
    "    filepath=EMBEDDING_PATH_ROOT+EN_EMBEDDING, \n",
    "    max_vocab=1000\n",
    ").filter_vocabulary({'cat', 'dog', 'house', 'tree', 'car'})\n",
    "print(f\"  Filtered vocab size: {len(test_loader)}\")\n",
    "print(f\"  Filtered words: {test_loader.get_vocabulary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t0ob3gl0w2j",
   "metadata": {},
   "source": [
    "## DictionaryParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ppsbhry56q",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dictionary pairs: 9197\n",
      "Filtered pairs (words with embeddings): 516\n",
      "Sample pairs: [('一些', 'some'), ('一代', 'generation'), ('一共', 'altogether'), ('一再', 'repeatedly'), ('一半', 'half'), ('一对', 'couple'), ('一带', 'region'), ('丈夫', 'husband'), ('下列', 'following'), ('不想', 'unexpectedly')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_234876/2311640738.py:9: UserWarning: Filtered out 94.4% of pairs (8681/9197). Only 516 pairs remain. Consider using embeddings with larger vocabulary or a different dictionary.\n",
      "  parser.filter_by_vocabulary(zh_vocab, en_vocab)\n"
     ]
    }
   ],
   "source": [
    "from vector_mapping import DictionaryParser\n",
    "\n",
    "# Parse MUSE dictionary with only simplified Chinese\n",
    "parser = DictionaryParser()\n",
    "parser.parse_muse_format('./dictionaries/cedict_processed.txt', include_traditional=False)\n",
    "print(f\"Total dictionary pairs: {len(parser.get_pairs())}\")\n",
    "\n",
    "# Filter by available embeddings\n",
    "parser.filter_by_vocabulary(zh_vocab, en_vocab)\n",
    "print(f\"Filtered pairs (words with embeddings): {len(parser.get_pairs())}\")\n",
    "\n",
    "# View sample pairs\n",
    "print(f\"Sample pairs: {parser.get_pairs()[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7y4364ix9t",
   "metadata": {},
   "source": [
    "## Cross-Lingual Translation Pair Similarities (Unmapped Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0zw0p7itqlqe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 10 translation pairs from dictionary:\n",
      "\n",
      "一些       <-> some                 : -0.0681\n",
      "一代       <-> generation           : 0.1055\n",
      "一共       <-> altogether           : -0.0749\n",
      "一再       <-> repeatedly           : -0.0315\n",
      "一半       <-> half                 : -0.0861\n",
      "一对       <-> couple               : -0.0835\n",
      "一带       <-> region               : 0.0474\n",
      "丈夫       <-> husband              : 0.0450\n",
      "下列       <-> following            : 0.1474\n",
      "不想       <-> unexpectedly         : 0.0292\n",
      "\n",
      "Results:\n",
      "  Pairs evaluated: 10/10\n",
      "  Average similarity: 0.0030\n",
      "  Std deviation: 0.0797\n",
      "  Min similarity: -0.0861\n",
      "  Max similarity: 0.1474\n"
     ]
    }
   ],
   "source": [
    "# Use translation pairs from the loaded dictionary parser\n",
    "pairs = parser.get_pairs()\n",
    "\n",
    "# Sample a subset if there are too many pairs (optional)\n",
    "sample_size = min(10, len(pairs))\n",
    "sampled_pairs = pairs[:sample_size]\n",
    "\n",
    "print(f\"Evaluating {sample_size} translation pairs from dictionary:\\n\")\n",
    "\n",
    "similarities = []\n",
    "found_count = 0\n",
    "\n",
    "for zh_word, en_word in sampled_pairs:\n",
    "    try:\n",
    "        # Get embeddings for both words\n",
    "        zh_vec = get_word_embedding(zh_word, zh_emb)\n",
    "        en_vec = get_word_embedding(en_word, en_emb)\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        sim = cosine_similarity(zh_vec, en_vec)\n",
    "        similarities.append(sim)\n",
    "        found_count += 1\n",
    "\n",
    "        print(f\"{zh_word:8s} <-> {en_word:20s} : {sim:.4f}\")\n",
    "    except KeyError as e:\n",
    "        # Skip pairs where word not in embeddings\n",
    "        continue\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Pairs evaluated: {found_count}/{sample_size}\")\n",
    "print(f\"  Average similarity: {np.mean(similarities):.4f}\")\n",
    "print(f\"  Std deviation: {np.std(similarities):.4f}\")\n",
    "print(f\"  Min similarity: {np.min(similarities):.4f}\")\n",
    "print(f\"  Max similarity: {np.max(similarities):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07d70a",
   "metadata": {},
   "source": [
    "## Cross-Lingual Translation Pair Similarities (Mapped Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pu49v1vkvql",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned Chinese vocabulary: 10000 words\n",
      "Sample words: ['，', '的', '。', '、', '在', '和', '：', '了', '是', '”']\n"
     ]
    }
   ],
   "source": [
    "# Load aligned Chinese embeddings\n",
    "ALIGNED_ZH_EMBEDDING = \"zh-aligned/merge-google/20251023-023520/sgns.merge.bin.gz\"\n",
    "\n",
    "zh_aligned_loader = EmbeddingLoader()\n",
    "zh_aligned_loader.load_word2vec(filepath=EMBEDDING_PATH_ROOT+ALIGNED_ZH_EMBEDDING, max_vocab=MAX_VOCAB)\n",
    "\n",
    "# Get aligned embeddings dict\n",
    "zh_aligned_emb = zh_aligned_loader.get_embeddings()\n",
    "\n",
    "print(f\"Aligned Chinese vocabulary: {len(zh_aligned_loader)} words\")\n",
    "print(f\"Sample words: {list(zh_aligned_emb.keys())[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nfbxzfmnfg",
   "metadata": {},
   "source": [
    "### Direct Comparison: Original vs Aligned Chinese Embeddings\n",
    "\n",
    "Now let's compare how well Chinese words match their English translations **before** and **after** alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "wlcl30ex8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: Original vs Aligned Chinese Embeddings\n",
      "\n",
      "Chinese    English         Original     Aligned      Improvement \n",
      "----------------------------------------------------------------------\n",
      "一          one                  0.1103      0.3351     +0.2248\n",
      "猫          cat                 -0.0078      0.6396     +0.6474\n",
      "狗          dog                 -0.0301      0.5386     +0.5687\n",
      "中国         China               -0.0480      0.4635     +0.5115\n",
      "美国         USA                  0.0400      0.3502     +0.3102\n"
     ]
    }
   ],
   "source": [
    "# Test specific translation pairs\n",
    "test_pairs = [\n",
    "    ('一', 'one'),\n",
    "    ('猫', 'cat'),\n",
    "    ('狗', 'dog'),\n",
    "    ('中国', 'China'),\n",
    "    ('美国', 'USA')\n",
    "]\n",
    "\n",
    "print(\"Comparison: Original vs Aligned Chinese Embeddings\\n\")\n",
    "print(f\"{'Chinese':<10} {'English':<15} {'Original':<12} {'Aligned':<12} {'Improvement':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for zh_word, en_word in test_pairs:\n",
    "    try:\n",
    "        # Get embeddings\n",
    "        zh_orig_vec = get_word_embedding(zh_word, zh_emb)\n",
    "        zh_aligned_vec = get_word_embedding(zh_word, zh_aligned_emb)\n",
    "        en_vec = get_word_embedding(en_word, en_emb)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        sim_original = cosine_similarity(zh_orig_vec, en_vec)\n",
    "        sim_aligned = cosine_similarity(zh_aligned_vec, en_vec)\n",
    "        improvement = sim_aligned - sim_original\n",
    "        \n",
    "        print(f\"{zh_word:<10} {en_word:<15} {sim_original:>11.4f} {sim_aligned:>11.4f} {improvement:>+11.4f}\")\n",
    "        \n",
    "    except KeyError as e:\n",
    "        print(f\"{zh_word:<10} {en_word:<15} {'NOT FOUND':^36}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ibe8wa404yk",
   "metadata": {},
   "source": [
    "### Translation Pairs Evaluation (Aligned Embeddings)\n",
    "\n",
    "Let's evaluate the same 10 dictionary translation pairs, but now using the **aligned** Chinese embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "s4gd2e0c6ho",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 10 translation pairs with ALIGNED Chinese embeddings:\\n\n",
      "一些       <-> some                 : 0.5504\n",
      "一代       <-> generation           : 0.5193\n",
      "一共       <-> altogether           : 0.0627\n",
      "一再       <-> repeatedly           : 0.4567\n",
      "一半       <-> half                 : 0.3925\n",
      "一对       <-> couple               : 0.3132\n",
      "一带       <-> region               : 0.4354\n",
      "丈夫       <-> husband              : 0.6911\n",
      "下列       <-> following            : 0.1347\n",
      "不想       <-> unexpectedly         : 0.0919\n",
      "\n",
      "Results (Aligned):\n",
      "  Pairs evaluated: 10/10\n",
      "  Average similarity: 0.3648\n",
      "  Std deviation: 0.2001\n",
      "  Min similarity: 0.0627\n",
      "  Max similarity: 0.6911\n",
      "\n",
      "============================================================\n",
      "COMPARISON: Original vs Aligned\n",
      "============================================================\n",
      "Original average similarity: 0.0030\n",
      "Aligned average similarity:  0.3648\n",
      "Improvement:                 +0.3618\n",
      "Improvement ratio:           119.8x\n"
     ]
    }
   ],
   "source": [
    "# Use the same translation pairs from the dictionary\n",
    "sampled_pairs = pairs[:sample_size]\n",
    "\n",
    "print(f\"Evaluating {sample_size} translation pairs with ALIGNED Chinese embeddings:\\\\n\")\n",
    "\n",
    "aligned_similarities = []\n",
    "found_count = 0\n",
    "\n",
    "for zh_word, en_word in sampled_pairs:\n",
    "    try:\n",
    "        # Get aligned Chinese embedding and English embedding\n",
    "        zh_aligned_vec = get_word_embedding(zh_word, zh_aligned_emb)\n",
    "        en_vec = get_word_embedding(en_word, en_emb)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        sim = cosine_similarity(zh_aligned_vec, en_vec)\n",
    "        aligned_similarities.append(sim)\n",
    "        found_count += 1\n",
    "        \n",
    "        print(f\"{zh_word:8s} <-> {en_word:20s} : {sim:.4f}\")\n",
    "    except KeyError as e:\n",
    "        # Skip pairs where word not in embeddings\n",
    "        continue\n",
    "\n",
    "print(f\"\\nResults (Aligned):\")\n",
    "print(f\"  Pairs evaluated: {found_count}/{sample_size}\")\n",
    "print(f\"  Average similarity: {np.mean(aligned_similarities):.4f}\")\n",
    "print(f\"  Std deviation: {np.std(aligned_similarities):.4f}\")\n",
    "print(f\"  Min similarity: {np.min(aligned_similarities):.4f}\")\n",
    "print(f\"  Max similarity: {np.max(aligned_similarities):.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"COMPARISON: Original vs Aligned\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Original average similarity: {np.mean(similarities):.4f}\")\n",
    "print(f\"Aligned average similarity:  {np.mean(aligned_similarities):.4f}\")\n",
    "print(f\"Improvement:                 {np.mean(aligned_similarities) - np.mean(similarities):+.4f}\")\n",
    "print(f\"Improvement ratio:           {np.mean(aligned_similarities) / np.mean(similarities) if np.mean(similarities) != 0 else float('inf'):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4jzj7a5o178",
   "metadata": {},
   "source": [
    "### Cross-Lingual Nearest Neighbors\n",
    "\n",
    "Now let's find the nearest neighbors across languages in the shared embedding space. We'll search for:\n",
    "1. **Chinese → English**: Find English words similar to Chinese concepts\n",
    "2. **English → Chinese**: Find Chinese words similar to English concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5flawxdmj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHINESE → ENGLISH (Find English words similar to Chinese concepts)\n",
      "======================================================================\n",
      "\n",
      "'猫' → English neighbors:\n",
      "  cat                  (0.6396)\n",
      "  dog                  (0.5551)\n",
      "  cats                 (0.5527)\n",
      "  pet                  (0.5153)\n",
      "  dogs                 (0.4624)\n",
      "\n",
      "'狗' → English neighbors:\n",
      "  dog                  (0.5386)\n",
      "  cat                  (0.5008)\n",
      "  dogs                 (0.4504)\n",
      "  pet                  (0.4393)\n",
      "  animal               (0.4348)\n",
      "\n",
      "'中国' → English neighbors:\n",
      "  China                (0.4635)\n",
      "  United_States        (0.4589)\n",
      "  Chinese              (0.3998)\n",
      "  India                (0.3960)\n",
      "  world                (0.3905)\n",
      "\n",
      "'学习' → English neighbors:\n",
      "  learning             (0.4125)\n",
      "  teaching             (0.4047)\n",
      "  teach                (0.3978)\n",
      "  math                 (0.3599)\n",
      "  taught               (0.3501)\n",
      "\n",
      "'美国' → English neighbors:\n",
      "  United_States        (0.5104)\n",
      "  U.S.                 (0.4505)\n",
      "  America              (0.4227)\n",
      "  Canada               (0.3988)\n",
      "  California           (0.3806)\n",
      "\n",
      "======================================================================\n",
      "ENGLISH → CHINESE (Find Chinese words similar to English concepts)\n",
      "======================================================================\n",
      "\n",
      "'cat' → Chinese neighbors:\n",
      "  猫                    (0.6396)\n",
      "  老鼠                   (0.5398)\n",
      "  狗                    (0.5008)\n",
      "  犬                    (0.4816)\n",
      "  宠物                   (0.4621)\n",
      "\n",
      "'dog' → Chinese neighbors:\n",
      "  猫                    (0.5551)\n",
      "  狗                    (0.5386)\n",
      "  犬                    (0.5333)\n",
      "  宠物                   (0.5045)\n",
      "  老鼠                   (0.4228)\n",
      "\n",
      "'China' → Chinese neighbors:\n",
      "  中国                   (0.4635)\n",
      "  越南                   (0.4363)\n",
      "  东南亚                  (0.4098)\n",
      "  我国                   (0.4098)\n",
      "  印尼                   (0.3935)\n",
      "\n",
      "'learning' → Chinese neighbors:\n",
      "  教学                   (0.4765)\n",
      "  学习                   (0.4125)\n",
      "  课程                   (0.4018)\n",
      "  读书                   (0.3659)\n",
      "  课堂                   (0.3633)\n",
      "\n",
      "'USA' → Chinese neighbors:\n",
      "  美国                   (0.3502)\n",
      "  瑞典                   (0.3475)\n",
      "  澳大利亚                 (0.3466)\n",
      "  意大利                  (0.3444)\n",
      "  新西兰                  (0.3360)\n"
     ]
    }
   ],
   "source": [
    "def find_nearest_neighbors(query_vec, target_embeddings, top_k=5):\n",
    "    \"\"\"\n",
    "    Find the top-k nearest neighbors in target embeddings to query vector.\n",
    "\n",
    "    Args:\n",
    "        query_vec: Query embedding vector\n",
    "        target_embeddings: Dict of {word: vector} to search in\n",
    "        top_k: Number of neighbors to return\n",
    "\n",
    "    Returns:\n",
    "        List of (word, similarity) tuples\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    for word, vec in target_embeddings.items():\n",
    "        sim = cosine_similarity(query_vec, vec)\n",
    "        similarities.append((word, sim))\n",
    "\n",
    "    # Sort by similarity (descending)\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "\n",
    "# Test Chinese → English (find English words similar to Chinese concepts)\n",
    "print(\"=\" * 70)\n",
    "print(\"CHINESE → ENGLISH (Find English words similar to Chinese concepts)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "zh_test_words = ['猫', '狗', '中国', '学习', '美国']\n",
    "\n",
    "for zh_word in zh_test_words:\n",
    "    if zh_word in zh_aligned_emb:\n",
    "        zh_vec = zh_aligned_emb[zh_word]\n",
    "        neighbors = find_nearest_neighbors(zh_vec, en_emb, top_k=5)\n",
    "\n",
    "        print(f\"\\n'{zh_word}' → English neighbors:\")\n",
    "        for word, sim in neighbors:\n",
    "            print(f\"  {word:20s} ({sim:.4f})\")\n",
    "    else:\n",
    "        print(f\"\\n'{zh_word}' not found in aligned embeddings\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENGLISH → CHINESE (Find Chinese words similar to English concepts)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "en_test_words = ['cat', 'dog', 'China', 'learning', 'USA']\n",
    "\n",
    "for en_word in en_test_words:\n",
    "    if en_word in en_emb:\n",
    "        en_vec = en_emb[en_word]\n",
    "        neighbors = find_nearest_neighbors(en_vec, zh_aligned_emb, top_k=5)\n",
    "\n",
    "        print(f\"\\n'{en_word}' → Chinese neighbors:\")\n",
    "        for word, sim in neighbors:\n",
    "            print(f\"  {word:20s} ({sim:.4f})\")\n",
    "    else:\n",
    "        print(f\"\\n'{en_word}' not found in English embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
